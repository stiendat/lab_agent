# ğŸ¤– Local AI Document Agent

A complete, locally-hosted AI document agent that automatically indexes your documents and provides intelligent question-answering capabilities using RAG (Retrieval-Augmented Generation).

## ğŸŒŸ Features

- **ğŸ” Auto-Indexing**: Automatically monitors directories and indexes new documents in real-time
- **ğŸ“š Multi-Format Support**: Handles PDF, Word documents (.docx), text files (.txt), and code files (.py, .js, .md, etc.)
- **ğŸ§  Semantic Search**: Uses sentence-transformers for high-quality 384-dimensional embeddings
- **ğŸ’¾ Dual Storage**: SQLite for metadata + Milvus Lite for vector storage
- **ğŸ¤– RAG-Powered Q&A**: Ask questions about your documents and get AI-powered answers with source citations
- **âš¡ Real-Time Processing**: File changes are detected and processed instantly
- **ğŸ–¥ï¸ Interactive CLI**: User-friendly command-line interface

## ğŸ“‹ Prerequisites

- **Python 3.8+** (tested on Python 3.12)
- **Local LLM Server**: llama.cpp or compatible OpenAI API server running on `localhost:8081`
- **Operating System**: macOS, Linux, or Windows

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
# Navigate to your project directory
cd /path/to/LAB_Agent

# Install dependencies (using uv - recommended)
uv sync

# OR using pip
pip install -r requirements.txt
```

### 2. Start Your Local LLM Server

Before running the agent, ensure you have a local LLM server running on port 8081:

```bash
# Example with llama.cpp
./llama-server --host localhost --port 8081 --model your-model.gguf

# The agent expects OpenAI-compatible API at:
# http://localhost:8081/v1/chat/completions
```

### 3. Run the Complete Agent

```bash
python complete_agent.py
```

This starts both:
- ğŸ” **Auto-indexing file watcher** (monitors `docs-to-watch-1/` and `docs-to-watch-2/`)
- ğŸ’¬ **Interactive query interface** (for asking questions)

### 4. Add Documents

Copy any documents to the monitored directories:
```bash
cp your-document.pdf docs-to-watch-1/
cp your-notes.txt docs-to-watch-2/
```

Files are automatically indexed within seconds! âš¡

### 5. Ask Questions

```
ğŸ’¬ Your question (or 'exit'/'stats'): What does the document say about machine learning?

ğŸ¤– AI Assistant:
Based on your documents, machine learning is described as...

ğŸ“š Sources (2 documents):
   1. your-document.pdf
   2. your-notes.txt
```

## ğŸ“ Project Structure

```
LAB_Agent/
â”œâ”€â”€ complete_agent.py           # ğŸ¯ Main entry point - runs everything
â”œâ”€â”€ agent_orchestrator.py       # ğŸ§  Core RAG logic and LLM integration
â”œâ”€â”€ auto_indexing_agent.py      # ğŸ“‚ File watcher + auto-indexing
â”œâ”€â”€ ingestion_pipeline.py       # ğŸ“„ Document processing and chunking
â”œâ”€â”€ embedding_generator.py      # ğŸ”¢ Text-to-vector conversion
â”œâ”€â”€ data_manager.py             # ğŸ’¾ SQLite + Milvus database management
â”œâ”€â”€ file_watcher.py             # ğŸ‘ï¸ Basic file system monitoring
â”œâ”€â”€ docs-to-watch-1/            # ğŸ“ Monitored directory 1
â”œâ”€â”€ docs-to-watch-2/            # ğŸ“ Monitored directory 2
â”œâ”€â”€ metadata.sqlite             # ğŸ—ƒï¸ Document metadata database
â””â”€â”€ milvus_local.db             # ğŸ” Vector embeddings database
```

## ğŸ› ï¸ Installation Details

### Required Dependencies

The project uses these key libraries:

```python
# Core AI/ML libraries
sentence-transformers==5.1.1    # Text embeddings
torch>=2.0.0                    # PyTorch backend

# Document processing
PyMuPDF                         # PDF processing
python-docx                     # Word document processing

# Vector database
pymilvus[milvus_lite]           # Vector storage and search

# File monitoring
watchdog                        # File system event monitoring

# HTTP requests
requests                        # LLM API communication
```

### Install with uv (Recommended)

```bash
# Install uv if you don't have it
pip install uv

# Install all dependencies
uv sync
```

### Install with pip

```bash
pip install sentence-transformers torch PyMuPDF python-docx "pymilvus[milvus_lite]" watchdog requests
```

## ğŸ® Usage Modes

### Mode 1: Complete Agent (Recommended)
```bash
python complete_agent.py
```
- âœ… Auto-indexing + Interactive queries
- âœ… Real-time file monitoring
- âœ… Best user experience

### Mode 2: Auto-Indexing Only
```bash
python auto_indexing_agent.py
```
- âœ… File monitoring and auto-indexing
- âœ… Interactive queries
- âŒ No concurrent operation

### Mode 3: Manual Orchestrator
```bash
python agent_orchestrator.py
```
- âœ… Interactive queries only
- âŒ No auto-indexing
- âŒ Manual file processing required

## ğŸ“– Commands

| Command | Description |
|---------|-------------|
| `[question]` | Ask any question about your documents |
| `stats` | View system statistics (documents, chunks, vectors) |
| `exit` | Gracefully exit the application |
| `Ctrl+C` | Emergency exit (both processes) |

## ğŸ”§ Configuration

### Monitored Directories

Default monitored directories:
- `./docs-to-watch-1/`
- `./docs-to-watch-2/`

To change monitored directories, modify `complete_agent.py`:

```python
# Custom directories
agent = CompleteDocumentAgent([
    "/path/to/your/documents",
    "/another/document/folder"
])
```

### LLM Server Configuration

Default LLM endpoint: `http://localhost:8081/v1/chat/completions`

To change the endpoint, modify `agent_orchestrator.py`:

```python
self.LLM_API_URL = "http://your-server:port/v1/chat/completions"
```

### Supported File Types

| Extension | Library Used | Status |
|-----------|--------------|---------|
| `.pdf` | PyMuPDF | âœ… Full support |
| `.docx` | python-docx | âœ… Full support |
| `.txt` | Built-in | âœ… Full support |
| `.md` | Built-in | âœ… Full support |
| `.py`, `.js`, `.html`, `.css`, etc. | Built-in | âœ… Full support |
| Other formats | Fallback | âš ï¸ Limited (filename only) |

## ğŸ“Š System Statistics

View real-time statistics by typing `stats`:

```
ğŸ“Š System Statistics:
   documents_indexed: 15
   total_chunks: 247
   total_vectors: 247
   embedding_dimension: 384
   llm_endpoint: http://localhost:8081/v1/chat/completions
```

## ğŸ› Troubleshooting

### Common Issues

**âŒ "Could not connect to LLM server"**
```bash
# Solution: Start your local LLM server first
./llama-server --host localhost --port 8081 --model your-model.gguf
```

**âŒ "ModuleNotFoundError: No module named 'sentence_transformers'"**
```bash
# Solution: Install dependencies
uv sync
# OR
pip install sentence-transformers
```

**âŒ "Database is locked"**
```bash
# Solution: Close any other instances of the agent
pkill -f "python.*complete_agent.py"
```

**âŒ Files not auto-indexing**
- âœ… Ensure files are copied to `docs-to-watch-1/` or `docs-to-watch-2/`
- âœ… Check console for error messages
- âœ… Verify file permissions

### Performance Tips

- **Large Documents**: PDFs with many pages may take longer to process
- **Batch Processing**: Copy multiple files at once for efficient indexing
- **Memory Usage**: The system loads a 384MB embedding model into memory
- **Storage**: Each document chunk uses ~1.5KB in the vector database

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   File Watcher  â”‚â”€â”€â”€â–¶â”‚ Ingestion Pipelineâ”‚â”€â”€â”€â–¶â”‚ Embedding Gen   â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Monitors dirs â”‚    â”‚ â€¢ Extract contentâ”‚    â”‚ â€¢ Textâ†’Vectors  â”‚
â”‚ â€¢ Detects changesâ”‚    â”‚ â€¢ Split chunks   â”‚    â”‚ â€¢ 384-dim       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚ Interactive CLI â”‚â—€â”€â”€â”€â”‚ Agent Orchestratorâ”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                 â”‚    â”‚                  â”‚
â”‚ â€¢ User queries  â”‚    â”‚ â€¢ RAG logic      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Display resultsâ”‚    â”‚ â€¢ LLM integrationâ”‚â”€â”€â”€â–¶â”‚   Data Manager  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                 â”‚
                                               â”‚ â€¢ SQLite (meta) â”‚
                                               â”‚ â€¢ Milvus (vectors)â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤ Contributing

This is a complete, self-contained AI document agent. Feel free to:

- Add support for more file formats
- Improve the embedding models
- Enhance the user interface
- Add web interface support

## ğŸ“ License

This project is provided as-is for educational and personal use.

## ğŸ¯ What's Next?

- ğŸŒ **Web Interface**: Add a web-based UI
- ğŸ”— **API Mode**: Expose REST API endpoints
- ğŸ“± **Mobile App**: Create mobile companion app
- ğŸ§ª **Advanced RAG**: Implement re-ranking and query expansion
- ğŸ”„ **Sync**: Multi-device document synchronization

---

## ğŸš€ Ready to Go!

Your AI document agent is now ready to help you unlock the knowledge in your document collection!

```bash
python complete_agent.py
```

Happy document querying! ğŸ¤–ğŸ“š
